{
    "title": "分类信息熵",
    "desc": "### 题目描述\n\n给定数据集，随机抽取一条数据，我们可以根据数据集每个分类的数量来得到该数据属于每个分类的概率。但我们该如何用一个数值来度量这种不确定性呢？答：信息熵。\n\n![](https://gss0.bdstatic.com/94o3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D244/sign=49cc170a35d3d539c53d08c70e86e927/2e2eb9389b504fc2554487f1e6dde71190ef6d2e.jpg)\n\n假设某个数据集中有两个分类，若各占一半，信息熵为 0.5；若分别占 1/4 和 3/4，信息熵约为 0.4，若分别占 1/10 和 9/10，信息熵约为 0.23，可以看到，分类越不均衡，信息熵越低，即可以代表数据集分类的不确定性越低。\n\nsklearn 自带数据集中有四个目标是分类的数据集：iris、digits、wine 和 breast_cancer。\n\n| 数据集        | 分类                                                         |\n| ------------- | ------------------------------------------------------------ |\n| iris          | 三种鸢尾属植物：Iris Setosa，Iris Versicolour，Iris Virginica |\n| digits        | 数字 0~9                                                     |\n| wine          | 三个起源地：class_0，class_1，class_2                        |\n| breast_cancer | 两种分类：良性 benign、恶性 malignant                        |\n\n现给定上述某数据集，请你用信息熵度量它的分类不确定性。\n\n### 输入描述\n\n输入为数据集的名称。\n\n### 输出描述\n\n输出它的分类信息熵，结果保留四位小数。\n\n### 测试样例\n\n#### 样例1: 输入-输出\n\n```\niris\n```\n\n```\n1.0986\n```\n\n",
    "templateCode": "",
    "sourceCode": "\nfrom sklearn import datasets\n\nfrom math import log\n\n\ndef entropy(x: list):\n    l = len(x)\n    ent = 0\n    for i in set(x):\n        p = x.count(i) / l\n        ent -= p * log(p)\n    return ent\n\n\nname = input()\n\ntarget = eval('datasets.load_' + name + '()')['target']\n\nprint('{:.4f}'.format(entropy(target.tolist())))\n",
    "lang": "Python3",
    "testCases": [
        {
            "input": "iris\n",
            "output": "1.0986\n"
        },
        {
            "input": "digits\n",
            "output": "2.3025\n"
        },
        {
            "input": "wine\n",
            "output": "1.0860\n"
        },
        {
            "input": "breast_cancer\n",
            "output": "0.6603\n"
        }
    ],
    "simple_desc": "计算数据集中分类的信息熵",
    "difficulty": "2",
    "tag": "决策树-信息熵",
    "author": "谢子聪"
}
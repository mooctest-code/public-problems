---
题目: 最佳分割点
简介: 计算不同分割下的分类条件熵
难度: 3
标签: 决策树|信息熵-条件熵
作者: 谢子聪
慕码: 3165/1807
---

### 题目描述

上一题，我们使用信息熵度量了数据集的分类不确定性。那么我们该如何降低这种不确定性？答：分割数据。

根据条件熵的定义和性质，有 `H(X)+H(Y|X) = H(X, Y) ≤ H(X)+H(Y)`，即 `H(Y|X) ≤ H(Y)`，其说明在给定 X 的条件下，Y 的信息熵将不变或减少。

![](https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=844315090,947533151&fm=15&gp=0.jpg)

因此，若我们能根据某条件，将数据集分割成多个子集，数据集的不确定性将不变或减少。

sklearn 自带的红酒数据集 wine 有 13 个连续性特征。现给定某特征，请你从该特征中选择一个数值 x 作为分割点，将数据集分割成 { ≤ x } 和 { > x } 的两部分，使得该数据集的不确定性最小。

### 输入描述

输入为 wine 数据集中某特征的名称。

### 输出描述

输出两行。第一行输出分割前的信息熵和分割后的条件熵，结果保留六位小数；第二行输出在给定特征下，将数据集分类不确定性最小化的特征值。

若有多个值，则两两间用空格隔开。

### 测试样例

#### 样例1: 输入-输出

```
alcohol
```

```
1.086038 0.705935
12.77
```

#### 样例2: 输入-输出

```
malic_acid
```

```
1.086038 0.883712
2.16
```

